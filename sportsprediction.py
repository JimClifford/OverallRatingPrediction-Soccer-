# -*- coding: utf-8 -*-
"""Group12_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HTyS5re2iyMuRpakFUUsZV1G_EvK1gX9

# DATA PREPARATION AND PREPROCESSING
"""

import numpy as np
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

data =pd.read_csv('/content/drive/My Drive/Colab Notebooks/MidsemesterProject/players_21.csv')
df = pd.DataFrame(data) #Creating the dataframe from csv
df

for column in df.columns: #Dropping the columns that have 30% and above missing values
    if df[column].isna().sum() >= 0.3*18944:
        df.drop(column, axis=1, inplace=True)
    else:
        pass

df

#Dropping the irrelevant columns(URLs, player pictures etc)
drop_columns = [
    'club_team_id', 'club_name', 'league_name', 'league_level',
    'club_position', 'club_jersey_number', 'club_loaned_from',
    'club_joined', 'club_contract_valid_until', 'nationality_id',
    'nationality_name', 'nation_team_id', 'nation_position',
    'nation_jersey_number', 'player_tags', 'goalkeeping_speed','real_face','body_type',
    'player_face_url', 'club_logo_url', 'club_flag_url','dob',
    'nation_logo_url', 'nation_flag_url','sofifa_id','player_url','short_name','long_name', 'player_traits'
]

for column in drop_columns:
  if column in df.columns:
    df.drop(column, axis=1, inplace=True)
df.info()

z = df['overall'] # Removing the dependant variable from the rest of the dataframe so that it is not affected by encoding, scaling
df.drop('overall', axis=1, inplace=True)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

numerical_columns = df.select_dtypes(exclude=['object']).columns
#imputing the numerical columns
df_imputed = df.copy()
imputer = SimpleImputer(strategy='mean')
df_imputed[numerical_columns] = imputer.fit_transform(df_imputed[numerical_columns])
df.drop(columns=numerical_columns, inplace=True)
df = pd.concat([df, df_imputed[numerical_columns]], axis=1)

#scaling the numerical columns
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[numerical_columns])
scaled_df = pd.DataFrame(scaled_data, columns=numerical_columns)
df.drop(columns=numerical_columns, inplace=True)
df = pd.concat([df, scaled_df], axis=1)
df.info()

# imputing categorical columns
object_df = df.select_dtypes(include=['object'])

for column in object_df.columns:
    mode_value = object_df[column].mode()[0]  # Calculate the mode value for the column
    object_df[column].fillna(mode_value, inplace=True)

# Update the original DataFrame 'df' with the imputed values
df.update(object_df)
df.info()

from sklearn.preprocessing import LabelEncoder # encoding the categorical columns
for column in object_df.columns:
    label = LabelEncoder()
    df[column] = label.fit_transform(df[column])

df.info()

"""# FEATURE ENGINEERING"""

df['overall'] = z # putting back the dependant variable back into the dataframe so as to find its correlation with the other columns

correlate = df.corr()

highly_correlated_columns = correlate.index[correlate['overall'] >= 0.6] # creating a new dataframe with columns that had a correlation of 0.6 and above with the dependant variable(overall)
new_df = df[highly_correlated_columns]
new_df.info()

y = new_df['overall'] #dropping the dependant variable from the new dataframe
new_df.drop('overall', axis=1, inplace=True)

"""# TRAINING MODELS AND EVALUTATION"""

from sklearn.model_selection import train_test_split

Xtrain, Xtest, Ytrain, Ytest = train_test_split(new_df, y, test_size=0.2, random_state=42) # splitting the data for training in the 80/20 ration

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error, mean_squared_error
rf = RandomForestRegressor(n_estimators=900, random_state=2)

rf.fit(Xtrain, Ytrain) # training with RandomForestRegressor
y_pred=rf.predict(Xtest)

rf.score(Xtest, Ytest)

mae = mean_absolute_error(Ytest, y_pred) # the mean absolute error
mse = mean_squared_error(Ytest, y_pred)
mse

import xgboost as xgb  # training with XGBRegressor
xgb_model1 = xgb.XGBRegressor(objective ='reg:linear',
                  n_estimators = 100, seed = 123)

# Fitting the model
xgb_model1.fit(Xtrain, Ytrain)
# Fitting the model
xgb_model1.fit(Xtrain, Ytrain)

# Predict the model
pred = xgb_model1.predict(Xtest)
xgb_model1.score(Xtest, Ytest)
# Predict the model
pred = xgb_model1.predict(Xtest)
program_conf = xgb_model1.score(Xtest, Ytest)

mae_xgb = mean_absolute_error(Ytest, pred) # the mean absolute error
mae_xgb

from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor()

gbr.fit(Xtrain, Ytrain) #training with GradientBoostingRegressor

ypred = gbr.predict(Xtest)

gbr.score(Xtest, Ytest)

mae_gbr = mean_absolute_error(Ytest, ypred)
mae_gbr

import xgboost as xgb
from sklearn.model_selection import GridSearchCV, KFold

# Assuming you have imported X and Y
# X: Your feature matrix
# Y: Your target variable

cv = KFold(n_splits=3)

PARAMETERS = {
    "max_depth": [2, 5, 6, 12],
    "n_estimators": [10, 50, 100]
}

xgb_reg = xgb.XGBRegressor()  # Use XGBoostRegressor instead of GradientBoostingClassifier

model_gs = GridSearchCV(xgb_reg, param_grid=PARAMETERS, cv=cv, scoring="neg_mean_squared_error")
model_gs.fit(Xtrain, Ytrain)

# Now, you can access the best parameters and best estimator as follows:
best_params = model_gs.best_params_
best_model = model_gs.best_estimator_
best_params

"""# TEST WITH NEW DATASET"""

data2 =pd.read_csv('/content/drive/My Drive/Colab Notebooks/MidsemesterProject/players_22.csv')
df2 = pd.DataFrame(data2)
df2['overall'].info(). # loading the new dataset

new_columns = []. #picking columns that are in the previous dataset used for training and putting them in a new dataset(player2_data) for training
for c in new_df.columns:
  new_columns.append(c)

player2_data = df2[new_columns]

player2_data
player2_y = df2['overall'] #dropping the dependant variable
df2.drop('overall',axis=1,inplace=True)

column_to_impute = ['release_clause_eur','passing' ]

df_imputed = player2_data.copy() #imputing the numerical columns
imputer = SimpleImputer(strategy='mean')
df_imputed[column_to_impute] = imputer.fit_transform(df_imputed[column_to_impute])
player2_data.drop(columns=column_to_impute, inplace=True)
new_player2_df= pd.concat([player2_data, df_imputed[column_to_impute]], axis=1)
#new_player2_df.info()

numerical_columns = new_player2_df.select_dtypes(exclude=['object']).columns # scaling the numerical columns

data_scaled = scaler.fit_transform(new_player2_df[numerical_columns])

player2_scaled_df = pd.DataFrame(data_scaled, columns=numerical_columns)
new_player2_df.drop(columns=numerical_columns, inplace=True)
new_player2_df= pd.concat([new_player2_df, player2_scaled_df], axis=1)
new_player2_df.info()

object_df_player2 = new_player2_df.select_dtypes(include=['object']).columns
 #encoding the categorical columns

for column in object_df_player2:
    label = LabelEncoder()
    new_player2_df[column] = label.fit_transform(new_player2_df[column])

new_player2_df.info()

new_player2_df.info()
new_X= new_player2_df
new_X.info()

X2train, X2test, Y2train, Y2test = train_test_split(new_X, player2_y, test_size=0.2, random_state=42) #splitting the data for training

import xgboost as xgb
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import GridSearchCV

# Define the XGBoost model
xgb_model2 = xgb.XGBRegressor(objective='reg:linear', n_estimators=1000)

# Hyperparameter tuning using Grid Search
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

grid_search = GridSearchCV(estimator=xgb_model2, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')
grid_search.fit(X2train, Y2train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Create a new XGBoost model with the best hyperparameters
best_xgb_model = xgb.XGBRegressor(objective='reg:linear', n_estimators=1000, **best_params)

# Fit the model to the training data
best_xgb_model.fit(X2train, Y2train)

# Predict using the optimized model
pred2 = best_xgb_model.predict(X2test)
program_conf2 = xgb_model1.score(Xtest, Ytest)

# Calculate the new MAE
mae_xgb_optimized = mean_absolute_error(Y2test, pred2)
print("Optimized MAE:", mae_xgb_optimized)

program_conf2

mae_xgb = mean_absolute_error(Y2test,pred2)


mae_xgb

"""# DEPLOYMENT PART 1"""

import pickle

filename1 = '/content/drive/My Drive/Colab Notebooks/MidsemesterProject/model1.sav'
pickle.dump(xgb_model1, open(filename1, 'wb'))

filename2 = '/content/drive/My Drive/Colab Notebooks/MidsemesterProject/model2.sav'
pickle.dump(best_xgb_model, open(filename2, 'wb'))

filename1 = '/content/drive/My Drive/Colab Notebooks/MidsemesterProject/model1.sav'

loaded_model1 = pickle.load(open(filename1, 'rb'))
loaded_model1

y_pred = loaded_model1.predict(Xtest)

filename2 = '/content/drive/My Drive/Colab Notebooks/MidsemesterProject/model2.sav'

loaded_model2 = pickle.load(open(filename2, 'rb'))
# loaded_model2
pred2 = loaded_model2.predict(X2test)

import joblib
from sklearn.preprocessing import StandardScaler

# Assuming you have trained your model and created the scaler object
scaler = StandardScaler()
scaler.fit(Xtrain)  # X_train is your training data

# Save the scaler to a file
joblib.dump(scaler, "/content/drive/My Drive/Colab Notebooks/MidsemesterProject/scaler.pkl")